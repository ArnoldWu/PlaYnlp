{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import PlaYnlp.tokenizer as tkr \n",
      "import PlaYnlp.vectorizer as vcr\n",
      "from PlaYnlp import dataio\n",
      "from PlaYnlp.analysis.heuristics.text_clustering import find_text_eps_neighborhood\n",
      "\n",
      "from PlaYnlp.sparse import L0_norm_col_summarizer as L0_col_sum\n",
      "from PlaYnlp.sparse import L1_norm_col_summarizer as L1_col_sum\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jieba_without_html_tokenizer = tkr.tokenize_gen(lambda text:tkr.jieba.cut(tkr.nltk.clean_html(text)))\n",
      "unigram_without_html_tokenizer = tkr.tokenize_gen(lambda text:tkr.ngram(tkr.nltk.clean_html(text),n=1))\n",
      "bigram_without_html_tokenizer = tkr.tokenize_gen(lambda text:tkr.ngram(tkr.nltk.clean_html(text),n=2))\n",
      "jieba_vec_count_kwargs = {\"tokenizer\":jieba_without_html_tokenizer,\"lowercase\":False}\n",
      "unigram_vec_count_kwargs = {\"tokenizer\":unigram_without_html_tokenizer,\"lowercase\":False}\n",
      "bigram_vec_count_kwargs = {\"tokenizer\":bigram_without_html_tokenizer,\"lowercase\":False}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles_df,articles_push_df = dataio.read_pickle_file(\"PTT___movie.pickle\")\n",
      "test_ptt_text_sdtm = dataio.read_pickle_file(\"sdtm_ptt_movie.pickle\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}